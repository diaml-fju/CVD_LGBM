import streamlit as st
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import shap

st.set_option("deprecation.showPyplotGlobalUse", False)

# ================== Sidebar ==================
st.sidebar.title("Model / page")
page = st.sidebar.selectbox("", ["CVD demo"])   # ä¹‹å¾Œè¦å¤šé å†åŠ 


# ================== å°å·¥å…·ï¼šæ‹¿ç‰¹å¾µå ==================
def get_feature_names_from_model_or_data(model, fallback_cols):
    """
    HistGradientBoostingClassifier æ²’æœ‰ get_booster()
    ä½†æœ‰ feature_names_in_ï¼ˆåªè¦æ˜¯ç”¨ DataFrame / æœ‰æ¬„ä½å fit çš„è©±ï¼‰
    é€™é‚Šå…ˆå˜—è©¦å¾ž model æ‹¿ï¼Œæ‹¿ä¸åˆ°å°±é€€å›žè¨“ç·´æª”çš„æ¬„ä½
    """
    if hasattr(model, "feature_names_in_"):
        return list(model.feature_names_in_)
    return list(fallback_cols)


# ================== å…±ç”¨ï¼šé æ¸¬ + SHAP ==================
def predict_and_explain(model, x_train, input_df, model_name="HGB"):
    st.subheader("Prediction")

    # 1) å°é½Šæ¬„ä½
    model_feature_names = get_feature_names_from_model_or_data(model, x_train.columns)
    input_df = input_df[model_feature_names]
    background = x_train[model_feature_names]

    # 2) é æ¸¬æ©ŸçŽ‡ï¼ˆHGB æ˜¯åˆ†é¡žçš„è©±ä¸€æ¨£æœ‰ predict_probaï¼‰
    proba = model.predict_proba(input_df)[0, 1]
    st.write(f"ðŸ”¢ Predicted probability: **{proba:.3f}**")

    # 3) è‡ªé©æ‡‰é–€æª»ï¼ˆä½ å¯ä»¥è‡ªå·±æ”¹æ•¸å­— / æ›å­—å…¸ï¼‰
    adaptive_thresholds = {
        "HGB": 0.14298505,
    }
    threshold = adaptive_thresholds.get(model_name, 0.5)

    if proba >= threshold:
        st.error(f"Predicted: **Positive** (prob >= {threshold:.3f})")
    else:
        st.success(f"Predicted: **Negative** (prob < {threshold:.3f})")

    # 4) SHAP è§£é‡‹
    st.subheader("SHAP explanation")

    # èƒŒæ™¯ä¸è¦å¤ªå¤šï¼Œä¸ç„¶ KernelExplainer æœƒå¾ˆæ…¢
    background_sample = background.sample(
        n=min(50, len(background)),
        random_state=42
    )

    # â˜… é‡é»žï¼š
    # SHAP çš„ TreeExplainer å° XGBoost / LightGBM / sklearn çš„ Tree / RF éƒ½å¾ˆ ok
    # ä½†å° HistGradientBoosting æœ‰æ™‚å€™æœƒç›´æŽ¥ä¸æ”¯æ´
    # æ‰€ä»¥é€™è£¡ã€Œå…ˆè©¦ã€ï¼Œå¤±æ•—å°±é€€å›ž KernelExplainer
    try:
        explainer = shap.TreeExplainer(model)
        shap_values = explainer(input_df)

        shap.plots.waterfall(shap_values[0], show=False)
        fig = plt.gcf()
        st.pyplot(fig)
        plt.close(fig)

    except Exception:
        # é€€å›žé€šç”¨ç‰ˆæœ¬
        explainer = shap.KernelExplainer(
            lambda x: model.predict_proba(x)[:, 1],
            background_sample
        )
        row = input_df.iloc[[0]]
        sv = explainer.shap_values(row)

        shap.plots.waterfall(
            shap.Explanation(
                values=sv[0],
                base_values=explainer.expected_value,
                data=row.values[0],
                feature_names=row.columns.tolist()
            ),
            show=False
        )
        fig = plt.gcf()
        st.pyplot(fig)
        plt.close(fig)


# ================== é é¢ï¼šä½ çš„é€™å€‹å°ˆæ¡ˆ ==================
def run_cvd_demo_page():
    st.title("CVD prediction (HistGradientBoostingClassifier)")

    # 1. è®€æ¨¡åž‹ï¼ˆé‡é»žï¼šsklearn çš„è¦ç”¨ pickle / joblib è®€ï¼Œä¸æ˜¯ load_modelï¼‰
    with open(r"CVD_HBB.joblib", "rb") as f:   # â† â† â† â‘  é€™è£¡æ›æˆä½ çš„æ¨¡åž‹è·¯å¾‘
        model = pickle.load(f)

    # 2. è®€è¨“ç·´è³‡æ–™ï¼Œå–®ç´”æ˜¯ç‚ºäº†æ‹¿æ¬„ä½çµæ§‹
    x = pd.read_csv(r"CVD_SHAP_Model.csv")     # â† â† â† â‘¡ é€™è£¡æ›æˆä½ çš„è¨“ç·´è³‡æ–™
    x_train = x.drop(columns=["Y"])            # â† â† â† â‘¢ å¦‚æžœä½ çš„ label ä¸æ˜¯å« Yï¼Œè¦æ”¹

    # 3. Streamlit è¼¸å…¥æ¬„ä½ï¼ˆé€™è£¡å…ˆæ”¾ä½ å‰›å‰›é‚£å¹¾å€‹ï¼‰
    st.write("### Input variables")
    NIHSS = st.number_input("NIHSS", min_value=0.0, value=1.0, step=0.1)
    HR_Max = st.number_input("HR_Max", min_value=0.0, value=85.0, step=0.1)
    BT_Mean = st.number_input("BT_Mean", min_value=0.0, value=36.2875, step=0.001)
    SBP_Mean = st.number_input("SBP_Mean", min_value=0.0, value=156.416667, step=0.1)
    BT_std = st.number_input("BT_std", min_value=0.0, value=0.309989919, step=0.001)

    # æŠŠç”¨æˆ¶è¼¸å…¥å…ˆä¸Ÿé€² dict
    user_inputs = {
        "NIHSS": NIHSS,
        "HR_Max": HR_Max,
        "BT_Mean": BT_Mean,
        "SBP_Mean": SBP_Mean,
        "BT_std": BT_std,
    }

    # 4. æŒ‰éˆ•è§¸ç™¼
    if st.sidebar.button("Analysis"):
        # ä¾ç…§è¨“ç·´è³‡æ–™æ¬„ä½é †åºçµ„ä¸€ç­†è³‡æ–™
        row = []
        for col in x_train.columns:
            row.append(user_inputs.get(col, 0.0))   # æ²’ç•«åœ¨ç•«é¢çš„æ¬„ä½å…ˆè£œ 0
        input_df = pd.DataFrame([row], columns=x_train.columns)

        # ä¿éšªèµ·è¦‹è½‰æˆ float
        input_df = input_df.astype(float)

        # ä¸ŸåŽ»è·‘é æ¸¬ + SHAP
        predict_and_explain(model, x_train, input_df, "HGB")


# ================== ä¸»æµç¨‹ ==================
if page == "CVD demo":
    run_cvd_demo_page()
